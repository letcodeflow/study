import tensorflow as tf
from sklearn.datasets import load_diabetes
from sklearn.model_selection import GridSearchCV,KFold,StratifiedKFold,train_test_split
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from xgboost import XGBClassifier,XGBRegressor
import time
import numpy as np
from sklearn.metrics import r2_score
from bayes_opt import BayesianOptimization
from xgboost import XGBRegressor
#1.data
datasets = load_diabetes()
x_data = datasets.data
y_data = datasets.target
print(x_data.shape, y_data.shape)
y_data = y_data.reshape(-1,1)
x_train, x_test, y_train, y_test = train_test_split(x_data,y_data,train_size=0.8,random_state=234)
print(datasets.target_filename)
x = tf.placeholder(tf.float32,shape=[None,10])
w = tf.Variable(tf.random_normal([10,1]))
b = tf.Variable(tf.random_normal([1,1]))
y = tf.placeholder(tf.float32,shape=[None,1])
hypothesis = tf.matmul(x,w)+b
loss = tf.reduce_mean(tf.square(hypothesis-y))
train = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(loss)

with tf.compat.v1.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(201):
        h_val = sess.run(hypothesis, feed_dict = {x:x_train,y:y_train})
        if i % 20 == 0:
            print(i, h_val)
    pred = sess.run(hypothesis, feed_dict={x:x_test})
    print(r2_score(y_test,pred))



# print('r2',r2_score(y_test,model.predict(x_test)))

# xgb = XGBRegressor(random_state=238)

# from sklearn.ensemble import BaggingClassifier,BaggingRegressor
# # model = BaggingRegressor(XGBRegressor(), n_estimators=100, random_state=23)

# start = time.time()
# model.fit(x_train,y_train)
# end = time.time()

# # print('params',model.best_params_)
# # print('score',model.best_score_)
# print('time',end-start)
# from sklearn.metrics import r2_score
# print(r2_score(y_test,(model.predict(x_test))))

# xgb grid
# score 0.2443777933346439
# time 3.1116368770599365
# 0.31370474417988337

# xgb bagging
# time 10.85739016532898
# 0.42723166481153063

# voting
# LinearRegression 정확도: 0.5119
# KNeighborsRegressor 정확도: 0.5014
# XGBRegressor 정확도: 0.2690
# LGBMRegressor 정확도: 0.3066
# CatBoostRegressor 정확도: 0.3867

# polinomaial 
# LinearRegression 정확도: 0.5046
# KNeighborsRegressor 정확도: 0.4908
# XGBRegressor 정확도: 0.3372
# LGBMRegressor 정확도: 0.3105
# CatBoostRegressor 정확도: 0.4095
# 0.4751770730755556

# xgb bo
# 'target': 0.35213061202013674

