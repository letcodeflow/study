from math import hypot
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston, load_breast_cancer,fetch_covtype,load_wine,load_iris,load_digits
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from bayes_opt import BayesianOptimization
from xgboost import XGBClassifier
#1. 데이터
datasets = load_wine()
import xgboost as xg
x_data = datasets.data
y_data = datasets.target
y_data = y_data.reshape(-1,1)
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder()
y_data = ohe.fit_transform(y_data).toarray()
print(x_data.shape,y_data.shape) #(178, 13) (178,)
x_train, x_test, y_train, y_test = train_test_split(x_data,y_data,train_size=0.8,random_state=234)
import tensorflow as tf
from sklearn.metrics import accuracy_score
#레이어1
x = tf.placeholder(tf.float32,shape=[None,13])
w = tf.Variable(tf.random_normal([13,3]))
b = tf.Variable(tf.random_normal([1,3]))
y = tf.placeholder(tf.float32,shape=[None,3])
hypothesis = tf.nn.softmax(tf.matmul(x,w)+b)
loss = tf.reduce_mean(-tf.reduce_mean(y*tf.log(hypothesis),axis=1))
train = tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(loss)
with tf.compat.v1.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(201):
        h_val = sess.run(hypothesis,feed_dict={x:x_train,y:y_train})
        if i %20 ==0:
            print(i, h_val)
    pred = sess.run(hypothesis,feed_dict={x:x_test})
    print(pred)
    pred = np.argmax(pred,axis=1).reshape(-1,1)
    y_test = np.argmax(y_test,axis=1)
    acc = accuracy_score(y_test,pred)
    print(acc)


# no pca lda
# 시간 0.5370004177093506
# 결과 0.9722222222222222

# pca 10
# x.shape(178, 13)
# 시간 0.5436191558837891
# 결과 0.9166666666666666

# lda 2
# y np.unique [0 1 2]
# 시간 0.5383071899414062
# 결과 1.0

# xgb
# 시간 1.0660011768341064
# 결과 1.0

# xgb bagging
# 시간 8.06783151626587
# 결과 1.0

# voting
# LogisticRegression 정확도: 1.0000
# KNeighborsClassifier 정확도: 1.0000
# XGBClassifier 정확도: 1.0000
# LGBMClassifier 정확도: 1.0000
# CatBoostClassifier 정확도: 1.0000

# xgb bo
# 'target': 0.9722222222222222