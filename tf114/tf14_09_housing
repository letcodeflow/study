import warnings
#1.데이터
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import r2_score
from bayes_opt import BayesianOptimization
from sklearn.model_selection import GridSearchCV,KFold,StratifiedKFold,train_test_split

from xgboost import XGBRegressor
datasets = fetch_california_housing()
x_data = datasets.data
y_data = datasets.target.reshape(-1,1)

print(x_data.shape, y_data.shape)
x_train, x_test, y_train, y_test = train_test_split(x_data,y_data,train_size=0.8,random_state=234)
import tensorflow as tf
x = tf.placeholder(tf.float32,shape=[None,8])
w = tf.Variable(tf.random_normal([8,1]))
b = tf.Variable(tf.random_normal([1,1]))
y = tf.placeholder(tf.float32,shape=[None,1])

hypothesis = tf.matmul(x,w)+b
loss = tf.reduce_mean(tf.square(hypothesis-y))
train = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(loss)

with tf.compat.v1.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(201):
        h_val = sess.run(hypothesis, feed_dict = {x:x_train, y:y_train})
        if i % 20 == 0:
            print(i, h_val)
    pred = sess.run(hypothesis, feed_dict={x:x_test})
    print(pred)
    print(y_test)
    print(r2_score(y_test,pred))
# import time
# start = time.time()
# model.fit(x_train,y_train)
# print(time.time()-start)
# print(model.score(x_test,y_test))

# xgb
# 1.052358865737915
# 0.8365610594961219

# bagging xgb
# 111.926518201828
# 0.8492910755039802

# voting
# LinearRegression 정확도: 0.6002
# KNeighborsRegressor 정확도: 0.7150
# XGBRegressor 정확도: 0.8365
# LGBMRegressor 정확도: 0.8417
# CatBoostRegressor 정확도: 0.8521.

# xgb bo
# 'target': 0.8350684993670087